---
layout: post
title: 【翻译】Neural Collaborative Filtering--神经协同过滤
tags: 推荐系统 神经网络
key: 20170422_trans
picture_frame: shadow
---
# 摘要
近年来，深层神经网络在语音识别，计算机视觉和自然语言处理方面都取得了巨大的成功。然而相对的，对应用深层神经网络的推荐系统的探索却受到较少的关注。在这项工作中，我们力求开发一种基于神经网络的技术，来解决在含有隐形反馈的基础上进行推荐的关键问题——协同过滤。<!--more-->

尽管最近的一些工作已经把深度学习运用到了推荐中，但是他们主要是用它（深度学习）来对一些辅助信息（auxiliary information）建模，比如描述文字的项目和音乐的声学特征。当涉及到建模协同过滤的关键因素（key factor）——用户和项目（item）特征之间的交互的时候，他们仍然采用矩阵分解的方式，并将内积（inner product）做为用户和项目的潜在特征点乘。通过用神经结构代替内积这可以从数据中学习任意函数，据此我们提出一种通用框架，我们称它为NCF（Neural network-based Collaborative Filtering，基于神经网络的协同过滤）。NCF是一种通用的框架，它可以表达和推广矩阵分解。为了提升NFC的非线性建模能力，我们提出了使用多层感知机去学习用户-项目之间交互函数（interaction function）。在两个数据集的广泛实验显示了我们提出的NCF框架对最先进的方法的显著改进。

# 关键字
协同过滤,神经网络,深度学习,矩阵分解,隐性反馈（Implicit Feedback）.

# 1 引言
在信息爆炸的时代，推荐系统在减轻信息过载方面发挥了巨大的作用，被众多在线服务，包括电子商务，网络新闻和社交媒体等广泛采用。个性化推荐系统的关键在于根据过去用户交互的内容（e.g. 评分，点击），对用户对项目的偏好进行建模，就是所谓的协同过滤[31,46]。在众多的协同过滤技术中，矩阵分解（MF）[14,21]是最受欢迎的，它将用户和项目映射到共享潜在空间（shared latent space），使用潜在特征向量（latent features），用以表示用户或项目。这样一来，用户在项目上的交互就被建模为它们潜在向量之间的内积。

　　由于Netflix Prize的普及，MF已经成为了潜在因素（latent factor）建模推荐的默认方法。已经有大量的工作致力于增强MF（的性能），例如将其与基于邻居（相邻用户or项目）的模型集成[21]，与项目内容的主题模型[38]相结合，还有将其扩展到因式分解机（factorization machines）[26]，以实现特征的通用建模。尽管MF对协同过滤有效，但众所周知，其性能可以被简单选择交互函数（内积）所阻碍。例如，对于显式反馈的评估预测任务，可以通过将用户和项目偏移项纳入交互函数来改善MF模型的性能。虽然内积算子[14]似乎只是一个微不足道的调整，但它指出了设计一个更好的专用交互函数，用于建模用户和项目之间的潜在特征交互的积极效果。简单地将潜在特征的乘积线性组合的内积可能不足以捕捉用户交互数据的复杂结构。

本文探讨了使用深层神经网络来学习数据的交互函数，而不是那些以前已经完成的手动工作（handcraft）[18,21]。神经网络已经被证明有拟合任何连续函数的能力[17]，最近深层神经网络（DNN）被发掘出在几个领域的出色表现：从计算机视觉，语音识别到文本处理[5,10,15,47]。然而，与广泛的MF方法文献相比，使用DNNs进行推荐的工作相对较少。虽然最近的一些研究进展[37,38,45]已经将DNN应用于推荐任务，并展示出不错的效果，但他们大多使用DNN来建模一些辅助信息，例如物品的文字描述，音乐的音频特征和图像的视觉内容（描述）。对于影响建模效果的关键因素——协同过滤，他们仍然采用MF，使用内积结合用户和项目潜在特征。

　　我们这项工作是通过形式化用于协同过滤的神经网络建模方法来解决上述研究问题。我们专注于隐性反馈（implicit feedback），通过参考观看视频，购买产品和点击项目等间接反映用户偏好的行为。与显性反馈（explicit feedback）（i.e. 评级和评论）相比，隐性反馈可以自动跟踪，从而更容易为内容提供商所收集。但是，由于隐性反馈不能反映用户的满意度，并且负反馈（negative feedback）存在自然稀疏（natural scarcity）问题，使得这个问题更具挑战性。在本文中，我们探讨了如何利用DNN来模拟噪声隐性反馈信号的中心问题。

　　这项工作的主要贡献有如下几点：

　　1. 我们提出了一种神经网络结构来模拟用户和项目的潜在特征，并设计了基于神经网络的协同过滤的通用框架NCF。

　　2. 我们表明MF可以被解释为NCF的特例，并利用多层感知器来赋予NCF高水平的非线性建模能力。

　　3. 我们对两个现实世界的数据集进行广泛的实验，以证明我们的NCF方法的有效性和对使用深度学习进行协作过滤的承诺。  

# 2 准备工作
我们首先把问题形式化，并讨论现有的隐性反馈协同过滤解决方案。然后，我们简要概括广泛使用的MF模型，重点指出使用内积所造成的对模型的限制。
## 2.1 学习隐性数据
令$M$和$N$分别表示用户和项目的数量。我们将从用户的隐性反馈得到的用户-项目交互矩阵 $ Y\in \mathbb{R}^{M\times N} $定义为:  

$$y_{ui}=\left\{
\begin{aligned}
&1, \bf if\ interaction(user\ u, item\ i)\ is\ observed.\\
&0, \bf otherwise
\end{aligned}
\right.(1)$$

这里 $y_{ui}$ 为 1 表示用户 $u$ 和项目 $i$ 存在交互记录;然而这并不意味着 $u$ 真的喜欢  $i$ 。同样的，值 0 也不是表明 $u$ 不喜欢 $i$ ，也有可能是这个用户根本不知道有这个项目。这对隐性反馈的学习提出了挑战，因为它提供了关于用户偏好的噪声信号。虽然观察到的条目至少反映了用户对项目的兴趣，但是未查看的条目可能只是丢失数据，并且这其中存在自然稀疏的负反馈。  

在隐性反馈上的推荐问题可以表达为估算矩阵 $Y$ 中未观察到的条目的分数问题（这个分数被用来评估项目的排名）。基于模型的方法假定这些缺失的数据可以由底层模型生成（或者说描述）。形式上它可以被抽象为学习函数$$\hat{y}_{ui}=f(u,i\vert\Theta)$$ ,其中  $$\hat{y}_{ui}$$ 表示交互 $$y_{ui}$$ 的预测分数，$$\Theta$$ 表示模型参数，$$f$$ 表示表示模型参数映射到预测分数的函数（我们把它称作交互函数）。

为了估计参数$\Theta$，现有的方法一般是遵循机器学习范例，优化目标函数。在文献中最常用到两种损失函数：逐点损失[14，19]（pointwise loss）和成对损失[27，33] (pairwise loss) 。由于显性反馈研究上丰富的工作的自然延伸[21，46]，在逐点的学习方法上通​​常是遵循的回归模式，最小化 $$\hat{y}_{ui}$$ 及其目标值 $$ y_{ui} $$之间的均方误差。同时为了处理没有观察到的数据，他们要么将所有未观察到的条目视作负反馈，要么从没有观察到条目中抽样作为负反馈实例[14]。对于成对学习[27，44]，做法是，观察到的条目应该比未观察到的那些条目的排名更高。因此，成对学习最大化观察项 $$\hat{y}_{ui}$$ 和未观察到的条目 $$\hat{y}_{ui}$$ 之间的空白，而不是减少 $$\hat{y}_{ui}$$ 和 $$y_{ui}$$ 之间的损失误差。

更进一步，我们的NCF框架利用神经网络,参数化相互作用函数 $f$,从而估计 $\hat{y}_{ui}$。因此，它就很自然地同时支持点损失和成对损失。

## 2.2 矩阵分解
MF(Matrix Factorization)用一个潜在特征向量实值将每个用户和项目关联起来。令 $$\mathbf{ \mathrm{p}}_{u}$$ 和 $$\mathbf{ \mathrm{q}}_{i}$$ 分别表示用户 $$u$$ 和项目 $$i$$ 的潜在向量；MF评估相互作用$$\ y_{ui}$$ 作为 $$\mathbf{ \mathrm{p}}_{u}$$ 和 $$\mathbf{ \mathrm{q}}_{i}$$ 的内积：

$$\hat{y}_{u，i}=f(u,i\mid \mathbf{\mathrm{p}_{u}},\mathbf{\mathrm{q}_{i}})=\mathbf{\mathrm{p}_{u}^{T}}\mathbf{\mathrm{q}_{i}}=\sum_{k=1}^{K}p_{uk}q_{ik},\ \ \ \ (2)$$

这里的 $K$ 表示潜在空间（latent space）的维度。正如我们所看到的，MF模型是用户和项目的潜在因素的双向互动，它假设潜在空间的每一维都是相互独立的并且用相同的权重将它们线性结合。因此，MF可视为潜在因素（latent factor）的线性模型。

![1.JPG](https://i.loli.net/2018/01/18/5a6035cfadd38.jpg)  

图1（Figure 1）展示了的内积函数（inner product function）如何限制MF的表现力。这里有两个背景必须事先说明清楚以便于更好地了解例子。第一点，由于MF将用户和项目映射到同一潜在空间中，两个用户之间的相似性也可以用内积，或者潜在向量之间的角度的余弦值来衡量<sup>1</sup>。第二点，不失一般性，我们使用Jaccard系数<sup>2</sup>作为MF需要恢复的两个用户的真实状况之间的相似度。
>1: 假定潜在向量都是单位长度   
>2：令 $$\boldsymbol{R}_{u}$$ 表示与用户 $$u$$ 交互的项目集，那么用户 $$u$$ 和 $$j$$ 之间的Jaccard相似系数就被定义为：
$$s_{ij}=\frac{\mid \boldsymbol{R}_{i}\mid \cap\mid \boldsymbol{R}_{j}\mid }{\mid \boldsymbol{R}_{i}\mid \cup\mid \boldsymbol{R}_{j}\mid }$$  

我们首先关注的图1(a)中的前三行（用户）。很容易可以计算出 $s_{23}(0.66)>s_{12}(0.5)>s_{13}(0.4)$ 。这样，$\bf p_{1}$，$\bf p_{2}$ 和 $\bf p_{3}$ 在潜在空间中的几何关系可绘制成图1(b)。现在，让我们考虑一个新的用户 $u_{4}$，它的输入在图1(a)中的用虚线框出。我们同样可以计算出 $s_{41}(0.6)>s_{43}(0.4)>s_{42}(0.2)$ ，表示 $u_{4}$ 最接近 $u_{1}$，接着是  $u_{3}$ ，最后是 $u_{2}$ 。然而，如果MF模型将 $\bf p_{4}$ 放在了 最接近 $\bf p_{1}$ 的位置（图1(b) 中的虚线展示了两种不同的摆放 $\bf p_{4}$ 的方式，结果一样），那么会使得 $\bf p_{4}$ 相比与 $\bf p_{3}$ 更接近于 $\bf p_{2}$ （显然，根据图1(a)，$u_{4}$ 应该更接近 $u_{3}$），这会导致很大的排名误差（ranking loss）。

上面的示例显示了MF因为使用一个简单的和固定的内积，来估计在低维潜在空间中用户-项目的复杂交互，从而所可能造成的限制。我们注意到，解决该问题的方法之一是使用大量的潜在因子 $K$ (就是潜在空间向量的维度)。然而这可能对模型的泛化能力产生不利的影响（e.g. 数据的过拟合问题），特别是在稀疏的集合上。在本文的工作中，我们通过使用DNNs从数据中学习交互函数，突破了这个限制。

# 3 神经协同过滤
我们首先提出的总体框架NCF，阐述NCF如何学习强调了隐式数据的二进制属性的概率模型。然后，我们展示了，MF能够表达为NCF 的推广（MF矩阵分解模型是NCF的一个特例）。我们探索DNNs在协同过滤上的应用，提出了NCF的一个实例，它采用了多层感知器（MLP）来学习用户-项目交互函数。最后，我们在NCF框架下结合了MF和MLP，提出了一种新的神经矩阵分解模型（neural matrix factorization model）;它统一了在建模用户项目潜在结构方面，MF的线性建模优势和MLP的非线性优势。

## 3.1 通用框架
为了允许神经网络对协同过滤进行一个完整的处理，我们采用图2（Figure 2）展示的多层感知机去模拟一个用户项目交互 $y_{ui}$ ，它的一层的输出作为下一层的输入。底部输入层包括两个特征向量 ${\bf v}_u^U$ 和 ${\bf v}_i^I$ ，分别用来描述用户 $u$ 和项目 $i$ 。 他们可以进行定制，用以支持广泛的用户和项目的建模，例如上下文感知[28，1]，基于内容[3]，和基于邻居的构建方式[26]。由于本文工作的重点是纯的协同过滤模型设置，我们仅使用一个用户和一个项目作为输入特征，它使用one-hot编码将它们转化为二值化稀疏向量。注意到，我们对输入使用这样的通用特征表示，可以很容易地使用的内容特征来表示用户和项目，以调整解决冷启动问题。

![2.JPG](https://i.loli.net/2018/01/18/5a607e0cd8775.jpg)

输入层上面是嵌入层（Embedding Layer）;它是一个全连接层，用来将输入层的稀疏表示映射为一个稠密向量（dense vector）。所获得的用户（项目）的嵌入（就是一个稠密向量）可以被看作是在潜在因素模型的上下文中用于描述用户（项目）的潜在向量。然后我们将用户嵌入和项目嵌入送入多层神经网络结构，我们把这个结构称为神经协作过滤层，它将潜在向量映射为预测分数。NCF层的每一层可以被定制，用以发现用户-项目交互的某些潜在结构。最后一个隐含层 $X$ 的维度尺寸决定了模型的能力。最终输出层是预测分数 $$\widehat{y}_{ui}$$ ，训练通过最小化 $$\widehat{y}_{ui}$$ 和其目标值 $$y_{ui}$$ 之间逐点损失进行。我们注意到，另一种方式来训练模型是通过成对学习，如使用个性化贝叶斯排名[27]和基于余量的损失（margin-based）[33]。由于本文的重点是对神经网络建模部分，我们将使用成对学习训练NCF留给今后的工作。  

当前我们制定的NCF预测模型如下：

$$\widehat{y}_{ui}=f({\bf P}^{T}{\bf v}_u^U,{\bf Q}^{T}{\bf v}_i^I|{\bf P},{\bf Q},\Theta_{f}),\ \ \ \ (3)$$

其中 ${\bf P}\in \mathbb{R}^{M\times K}$， ${\bf Q}\in \mathbb{R}^{N\times K}$，分别表示用户和项目的潜在因素矩阵；$\Theta_{j}$ 表示交互函数 $f$ 的模型参数。由于函数 $f$ 被定义为多层神经网络，它可以被定制为：

$$f({\bf P}^{T}{\bf v}_u^U,{\bf Q}^{T}{\bf v}_i^I)=\phi_{out}(\phi_{X}(...\phi_{2}(\phi_{1}({\bf P}^{T}{\bf v}_u^U,{\bf Q}^{T}{\bf v}_i^I))...)),\ \ \ \ (4)$$  

其中 $\phi_{out}$ 和 $\phi_{x}$ 分别表示为输出层和第 $x$ 个神经协作过滤（CF）层映射函数，总共有 $X$ 个神经协作过滤（CF）层。

### 3.1.1 NCF学习
学习模型参数，现有的逐点学习方法[14,39]主要运用均方误差（squared loss）进行回归：  

$$L_{sqr}=\sum_{(u,i)\in{\bf{y}\cup\bf{y^{-}}}}w_{ui}(y_{ui}-\widehat{y}_{ui})^{2},\ \ \ \ (5)$$

其中 $\bf{y}$ 表示交互矩阵 $\bf Y$ 中观察到的条目（如对电影有明确的评分，评级）， $\bf{y^{-}}$  表示消极实例（negative instances，可以将未观察的样本全体视为消极实例，或者采取抽样的方式标记为消极实例）; $w_{ui}$ 是一个超参数，用来表示训练实例 $(u,i)$ 的权重。虽然均方误差可以通过假设观测服从高斯分布[29]来作出解释，我们仍然指出，它不适合处理隐性数据（implicit data）。这是因为对于隐含数据来说，目标值 $y_{ui}$ 是二进制值1或0，表示 $u$ 是否与 $i$ 进行了互动。在下文中，我们提出了逐点学习NCF的概率学方法，特别注重隐性数据的二进制属性。

考虑到隐性反馈的一类性质，我们可以将 $y_{ui}$ 的值作为一个标签——1表示项目 $i$ 和用户 $u$ 相关，否则为0。这样一来预测分数 $$\widehat{y}_{ui}$$ 就代表了项目 $i$ 和用户 $u$ 相关的可能性大小。为了赋予NCF这样的概率解释，我们需要将网络输出限制到[0，1]的范围内，通过使用概率函数（e.g. 逻辑函数sigmoid或者probit函数）作为激活函数作用在输出层 $\phi_{out}$ ，我们可以很容易地实现数据压缩。经过以上设置后，我们这样定义似然函数：

$$p\left(\bf{y},\bf{y^{-}}|{\bf P},{\bf Q},\Theta_f\right)=\prod_{(u,i)\in\bf{y}}\widehat{y}_{ui}\prod_{(u,i)\in\bf{y}^{-}}\left(1-\widehat{y}_{ui}\right).\ \ \ \ (6)$$

对似然函数取负对数，我们得到（负对数可以用来表示Loss函数，而且还能消除小数乘法的下溢出问题）:  

$$L=-\sum_{(u,i)\in\bf{y}}\log\widehat{y}_{ui}-\sum_{(u,i)\in\bf{y}^{-}}\log\left(1-\widehat{y}_{ui}\right)=\sum_{(u,i)\in\bf{y}\cup\bf{y}^{-}}y_{ui}\log\widehat{y}_{ui}+\left(1-y_{ui}\right)\log\left(1-\widehat{y}_{ui}\right).\ \ \ \ (7)$$  

这是NCF方法需要去最小化的目标函数，并且可以通过使用随机梯度下降（SGD）来进行训练优化。细心的读者可能发现了，这个函数和二类交叉熵损失函数（binary cross-entropy loss，又被成为log loss）是一样的。通过在NCF上使用这样一个概率处理（probabilistic treatment），我们把隐性反馈的推荐问题当做一个二分类问题来解决。由于分类用的交叉熵损失很少出现在有关推荐的文献中，我们将在这项工作中对它进行探讨，并在4.3节展示它的有效性。对于消极实例 $\bf{y}^{-}$ ，我们在每次迭代均匀地从未观察到的相互作用中采样（作为消极实例）并且对照可观察到交互的数量，控制采样比率。虽然非均匀采样策略（例如，基于项目流行度进行采样[14,12]）可能会进一步提高模型性能，我们将这方面的探索作为今后的工作。

## 3.2 广义矩阵分解
我们现在来证明MF是如何被解释为我们的NCF框架的一个特例。由于MF是推荐领域最流行的模型，并已在众多文献中被广泛的研究，复现它能证明NCF可以模拟大部分的分解模型[26]。由于输入层是用户（项目）ID中的一个one-hot encoding编码，所获得的嵌入向量可以被看作是用户（项目）的潜在向量。我们用 $${\bf{P}}^{T}{\bf{v}}_u^ U$$ 表示用户的潜在向量 $${\bf{p}}_{u}$$ ，$${\bf{Q}}^{T}{\bf{v}}_i^ I$$ 表示项目的潜在向量 $${\bf{q}}_{i}$$ ,我们定义第一层神经CF层的映射函数为：

$$\phi_{1}\left({\bf{p}}_{u},{\bf{q}}_{i}\right)={\bf{p}}_{u}\odot{\bf{q}}_{i},\ \ \ \ (8)$$

其中 $\odot$ 表示向量的逐元素乘积。然后，我们将向量映射到输出层：

$$\widehat{y}_{ui}=a_{out}\left({\bf h}^{T}\left({\bf{p}}_{u}\odot{\bf{q}}_{i} \right)\right),\ \ \ \ (9)$$

其中 $a_{out}$ 和 ${\bf h}$ 分别表示输出层的激活函数和连接权。直观地讲，如果我们将 $a_{out}$ 看做一个恒等函数， ${\bf h}$ 权重全为1，显然这就是我们的MF模型。在NCF的框架下，MF可以很容易地被泛化和推广。例如，如果我们允许从没有一致性约束（uniform constraint）的数据中学习  ${\bf h}$ ，则会形成MF的变体，它允许潜在维度的不同重要性（这句话不好翻译，原文放在这里For example, if we allow ${\bf h}$  to be learnt from data without the uniform constraint, it will result in a variant of MF that allows varying importance of latent dimensions）。如果我们用一个非线性函数 $a_{out}$ ，将进一步推广MF到非线性集合，使得模型比线性MF模型更具有表现力。在本文的工作中，我们在NCF下实现一个更一般化的MF，它使用Sigmoid函数 $σ(x)=1/(1+e^{-x})$ 作为激活函数，通过log loss(第3.1.1节)学习 ${\bf h}$ 。我们称她为GMF（Generalized Matrix Factorization，广义矩阵分解）。

## 3.3 多层感知机(MLP)
于NCF用两条路线来对用户和项目建模（图2中可以明显看出用户和项目两个输入），自然地，需要通过两个路线，把他们各自的特征连接结合起来。这种设计已经在多模态深度学习工作中[47，34]被广泛采用。然而，简单地对向量的连接不足以说明用户和项目之间的潜在特征，这对协同过滤建模来说是不够的。为了解决这个问题，我们提出在向量连接上增加隐藏层，使用标准的MLP（多层感知机）学习用户和项目潜在特征之间的相互作用。在这个意义上，我们可以赋予模型高水平的灵活性和非线性建模能力，而不是GMF（广义矩阵分解）那样的简单使用逐元素相乘的内积来描述用户和项目之间的潜在交互特征。更确切地说，我们的NCF框架下的MLP模型定义为：


$$
\bf{z}_{1}=\phi_{1}({{\bf{p}}_{u}},{{\bf{q}}_{i}})=[{\bf{p}}_{u},{\bf{q}}_{i}]^ T \\
\phi_{2}({\bf{z}}_{1})=a_{2}\left({\bf{W}}_2^T{\bf{z}}_{1}+{\bf b}_{2}\right),\\
......\\
\phi_{L}({\bf{z}}_{L-1})=a_{L}\left({\bf{W}}_L^T{\bf{z}}_{L-1}+{\bf b}_{L}\right),\\
\widehat{y}_{ui}=\sigma\left({\bf{h}}^{T}\phi_{L}\left({\bf{z}}_{L-1}\right)\right),\ \ \ \ \ \ \ \ \ \ (10)$$

这里的 $${\bf W}_{x}$$, $${\bf b}_{x}$$ 和 $$a_{x}$$ 分别表示 $$x$$ 层的感知机中的的权重矩阵，偏置向量（神经网络的神经元阈值）和激活函数。对于MLP层的激活函数，可以选择sigmoid，双曲正切（tanh）和ReLU，等等。我们分析一下每个函数：1)sigmoid函数将每个神经元的输出限制在（0,1），这有可能限制该模型的性能;并且它存在过饱和的问题，当输出接近1或者0的时候，神经元就会陷入停止学习的困境（这里应该指的是“早停的问题”）。2)虽然双曲正切是一个更好的选择，并已被广泛使用[6，44]，但它只是在一定程度上缓和了sigmoid的问题，因为它可以被看作是sigmoid的缩放的版本（$$tanh(x/2)=2σ(x)-1$$）。3)因此，我们选择ReLU，它更具生物合理性（biologically plausible），并且已经被证明不会导致过饱和[9];此外，它支持稀疏的激活（sparse activations），非常适合稀疏的数据，使模型不至于过拟合。我们的实验结果表明，ReLU的表现略好于双曲正切函数tanh和sigmoid。

至于网络结构的设计，一种常见的解决方案是设计一个塔式模型，其中，底层是最宽的，并且每个相继的层具有更少的神经元数量（如图2（Figure 2））。（设计这种结构的）前提是，通过在更高层使用少量的隐藏单元，它们可以从数据中学习到更多的抽象特征[10]。根据经验，我们搭建这样的塔结构：对于更高的层，相比于之前一层，缩减一半规模。

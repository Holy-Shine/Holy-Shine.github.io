---
layout: post
title: 【翻译】Neural Collaborative Filtering--神经协同过滤
tags: 推荐系统 神经网络
key: 20170422_trans
picture_frame: shadow
---
# 摘要
近年来，深层神经网络在语音识别，计算机视觉和自然语言处理方面都取得了巨大的成功。然而相对的，对应用深层神经网络的推荐系统的探索却受到较少的关注。在这项工作中，我们力求开发一种基于神经网络的技术，来解决在含有隐形反馈的基础上进行推荐的关键问题——协同过滤。

尽管最近的一些工作已经把深度学习运用到了推荐中，但是他们主要是用它（深度学习）来对一些辅助信息（auxiliary information）建模，比如描述文字的项目和音乐的声学特征。当涉及到建模协同过滤的关键因素（key factor）——用户和项目（item）特征之间的交互的时候，他们仍然采用矩阵分解的方式，并将内积（inner product）做为用户和项目的潜在特征点乘。通过用神经结构代替内积这可以从数据中学习任意函数，据此我们提出一种通用框架，我们称它为NCF（Neural network-based Collaborative Filtering，基于神经网络的协同过滤）。NCF是一种通用的框架，它可以表达和推广矩阵分解。为了提升NFC的非线性建模能力，我们提出了使用多层感知机去学习用户-项目之间交互函数（interaction function）。在两个数据集的广泛实验显示了我们提出的NCF框架对最先进的方法的显著改进。

# 关键字
协同过滤,神经网络,深度学习,矩阵分解,隐性反馈（Implicit Feedback）.

# 1. 引言
在信息爆炸的时代，推荐系统在减轻信息过载方面发挥了巨大的作用，被众多在线服务，包括电子商务，网络新闻和社交媒体等广泛采用。个性化推荐系统的关键在于根据过去用户交互的内容（e.g. 评分，点击），对用户对项目的偏好进行建模，就是所谓的协同过滤[31,46]。在众多的协同过滤技术中，矩阵分解（MF）[14,21]是最受欢迎的，它将用户和项目映射到共享潜在空间（shared latent space），使用潜在特征向量（latent features），用以表示用户或项目。这样一来，用户在项目上的交互就被建模为它们潜在向量之间的内积。

　　由于Netflix Prize的普及，MF已经成为了潜在因素（latent factor）建模推荐的默认方法。已经有大量的工作致力于增强MF（的性能），例如将其与基于邻居（相邻用户or项目）的模型集成[21]，与项目内容的主题模型[38]相结合，还有将其扩展到因式分解机（factorization machines）[26]，以实现特征的通用建模。尽管MF对协同过滤有效，但众所周知，其性能可以被简单选择交互函数（内积）所阻碍。例如，对于显式反馈的评估预测任务，可以通过将用户和项目偏移项纳入交互函数来改善MF模型的性能。虽然内积算子[14]似乎只是一个微不足道的调整，但它指出了设计一个更好的专用交互函数，用于建模用户和项目之间的潜在特征交互的积极效果。简单地将潜在特征的乘积线性组合的内积可能不足以捕捉用户交互数据的复杂结构。

本文探讨了使用深层神经网络来学习数据的交互函数，而不是那些以前已经完成的手动工作（handcraft）[18,21]。神经网络已经被证明有拟合任何连续函数的能力[17]，最近深层神经网络（DNN）被发掘出在几个领域的出色表现：从计算机视觉，语音识别到文本处理[5,10,15,47]。然而，与广泛的MF方法文献相比，使用DNNs进行推荐的工作相对较少。虽然最近的一些研究进展[37,38,45]已经将DNN应用于推荐任务，并展示出不错的效果，但他们大多使用DNN来建模一些辅助信息，例如物品的文字描述，音乐的音频特征和图像的视觉内容（描述）。对于影响建模效果的关键因素——协同过滤，他们仍然采用MF，使用内积结合用户和项目潜在特征。

　　我们这项工作是通过形式化用于协同过滤的神经网络建模方法来解决上述研究问题。我们专注于隐性反馈（implicit feedback），通过参考观看视频，购买产品和点击项目等间接反映用户偏好的行为。与显性反馈（explicit feedback）（i.e. 评级和评论）相比，隐性反馈可以自动跟踪，从而更容易为内容提供商所收集。但是，由于隐性反馈不能反映用户的满意度，并且负反馈（negative feedback）存在自然稀疏（natural scarcity）问题，使得这个问题更具挑战性。在本文中，我们探讨了如何利用DNN来模拟噪声隐性反馈信号的中心问题。

　　这项工作的主要贡献有如下几点：

　　1. 我们提出了一种神经网络结构来模拟用户和项目的潜在特征，并设计了基于神经网络的协同过滤的通用框架NCF。

　　2. 我们表明MF可以被解释为NCF的特例，并利用多层感知器来赋予NCF高水平的非线性建模能力。

　　3. 我们对两个现实世界的数据集进行广泛的实验，以证明我们的NCF方法的有效性和对使用深度学习进行协作过滤的承诺。  

# 2. 准备工作
我们首先把问题形式化，并讨论现有的隐性反馈协同过滤解决方案。然后，我们简要概括广泛使用的MF模型，重点指出使用内积所造成的对模型的限制。
## 2.1 学习隐性数据
令$M$和$N$分别表示用户和项目的数量。我们将从用户的隐性反馈得到的用户-项目交互矩阵$Y\in \mathbb{R}^{M\times N} $定义为:  

$$
y_{ui}=\left\{
  \begin{aligned}
  &1, \bf if\ interaction(user\ u, item\ i)\ is\ observed.\\
  &0, \bf otherwise
  \end{aligned}
\right. (1)$$

这里 $y_{ui}$ 为 1 表示用户 $u$ 和项目 $i$ 存在交互记录;然而这并不意味着 $u$ 真的喜欢  $i$ 。同样的，值 0 也不是表明 $u$ 不喜欢 $i$ ，也有可能是这个用户根本不知道有这个项目。这对隐性反馈的学习提出了挑战，因为它提供了关于用户偏好的噪声信号。虽然观察到的条目至少反映了用户对项目的兴趣，但是未查看的条目可能只是丢失数据，并且这其中存在自然稀疏的负反馈。  

在隐性反馈上的推荐问题可以表达为估算矩阵 $Y$ 中未观察到的条目的分数问题（这个分数被用来评估项目的排名）。基于模型的方法假定这些缺失的数据可以由底层模型生成（或者说描述）。形式上它可以被抽象为学习函数 $\hat{y}_{ui}=f(u,i \vert \Theta)$ ,其中 $\hat{y}_{ui}$ 表示交互 $y_{ui}$ 的预测分数，$\Theta$ 表示模型参数，$f$ 表示表示模型参数映射到预测分数的函数（我们把它称作交互函数）。

为了估计参数$\Theta$，现有的方法一般是遵循机器学习范例，优化目标函数。在文献中最常用到两种损失函数：逐点损失[14，19]（pointwise loss）和成对损失[27，33] (pairwise loss) 。由于显性反馈研究上丰富的工作的自然延伸[21，46]，在逐点的学习方法上通​​常是遵循的回归模式，最小化 $\hat{y}_{ui}$ 及其目标值 $ y_{ui} $之间的均方误差。同时为了处理没有观察到的数据，他们要么将所有未观察到的条目视作负反馈，要么从没有观察到条目中抽样作为负反馈实例[14]。对于成对学习[27，44]，做法是，观察到的条目应该比未观察到的那些条目的排名更高。因此，成对学习最大化观察项 $\hat{y}_{ui}$ 和未观察到的条目 $\hat{y}_{ui}$ 之间的空白，而不是减少 $\hat{y}_{ui}$ 和 $\ y_{ui}$ 之间的损失误差。

更进一步，我们的NCF框架利用神经网络,参数化相互作用函数 $f$,从而估计 $\hat{y}_{ui}$。因此，它就很自然地同时支持点损失和成对损失。

## 2.2 矩阵分解
MF(Matrix Factorization)用一个潜在特征向量实值将每个用户和项目关联起来。令 $\mathbf{ \mathrm{p}}_{u}$ 和 $\mathbf{ \mathrm{q}}_{i}$ 分别表示用户 $u$ 和项目 $i$ 的潜在向量；MF评估相互作用$\ y_{ui}$ 作为 $\mathbf{ \mathrm{p}}_{u}$ 和 $\mathbf{ \mathrm{q}}_{i}$ 的内积：

$$\hat{y}_{u，i}=f(u,i\mid \mathbf{\mathrm{p}_{u}},\mathbf{\mathrm{q}_{i}})=\mathbf{\mathrm{p}_{u}^{T}}\mathbf{\mathrm{q}_{i}}=\sum_{k=1}^{K}p_{uk}q_{ik},\ \ \ \ (2)$$

这里的 $K$ 表示潜在空间（latent space）的维度。正如我们所看到的，MF模型是用户和项目的潜在因素的双向互动，它假设潜在空间的每一维都是相互独立的并且用相同的权重将它们线性结合。因此，MF可视为潜在因素（latent factor）的线性模型。

![1.JPG](https://i.loli.net/2018/01/18/5a6035cfadd38.jpg)  

图1（Figure 1）展示了的内积函数（inner product function）如何限制MF的表现力。这里有两个背景必须事先说明清楚以便于更好地了解例子。第一点，由于MF将用户和项目映射到同一潜在空间中，两个用户之间的相似性也可以用内积，或者潜在向量之间的角度的余弦值来衡量<sup>1</sup>。第二点，不失一般性，我们使用Jaccard系数<sup>2</sup>作为MF需要恢复的两个用户的真实状况之间的相似度。
>1: 假定潜在向量都是单位长度   
>2：令 $\boldsymbol{R}_{u}$ 表示与用户 $u$ 交互的项目集，那么用户 $u$ 和 $j$ 之间的Jaccard相似系数就被定义为：
$s_{ij}=\frac{\mid \boldsymbol{R}_{i}\mid \cap\mid \boldsymbol{R}_{j}\mid }{\mid \boldsymbol{R}_{i}\mid \cup\mid \boldsymbol{R}_{j}\mid }$  

我们首先关注的图1(a)中的前三行（用户）。很容易可以计算出 $s_{23}(0.66)>s_{12}(0.5)>s_{13}(0.4)$ 。这样，$\bf p_{1}$，$\bf p_{2}$ 和 $\bf p_{3}$ 在潜在空间中的几何关系可绘制成图1(b)。现在，让我们考虑一个新的用户 $u_{4}$，它的输入在图1(a)中的用虚线框出。我们同样可以计算出 $s_{41}(0.6)>s_{43}(0.4)>s_{42}(0.2)$ ，表示 $u_{4}$ 最接近 $u_{1}$，接着是  $u_{3}$ ，最后是 $u_{2}$ 。然而，如果MF模型将 $\bf p_{4}$ 放在了 最接近 $\bf p_{1}$ 的位置（图1(b) 中的虚线展示了两种不同的摆放 $\bf p_{4}$ 的方式，结果一样），那么会使得 $\bf p_{4}$ 相比与 $\bf p_{3}$ 更接近于 $\bf p_{2}$ （显然，根据图1(a)，$u_{4}$ 应该更接近 $u_{3}$），这会导致很大的排名误差（ranking loss）。

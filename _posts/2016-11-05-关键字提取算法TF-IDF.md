---
layout: post
title: 关键字提取算法TF-IDF
tags: 自然语言理解
key: 20161105_TFIDF
picture_frame: shadow
---
在文本分类的学习过程中，在“如何衡量一个关键字在文章中的重要性”的问题上，需要某个属性来对文本进行量化，就是今天要讲的$TF-IDF$。<!--more-->
$TF-IDF$，理解起来相当简单，他实际上就是$TF\times IDF$，两个计算值的乘积，用来衡量一个词库中的词对每一篇文档的重要程度。下面我们分开来讲这两个值，$TF$和$IDF$。
## TF
$TF$，是` Term Frequency `的缩写，就是某个关键字出现的频率，具体来讲，就是词库中的某个词在当前文章中出现的频率。那么我们可以写出它的计算公式：  

$$TF_{i,j}=\frac{n_{i,j}}{\sum_kn_{i,k}}$$  

其中：
>$TF_{i,j}$: 关键词 $j$ 在文档 $i$ 中出现的频率。   
>$n_{i,j}$: 关键词 $j$ 在文档 $i$ 中出现的次数。  

举个例子。一篇文章一共 100 个词汇，其中`"机器学习"`一共出现 10 次， 那么它的 $TF=\frac{10}{100}=0.1$。  

这么看起来好像仅仅使用一个 $TF$ 就能用来评估一个关键词的重要性（出现频率越高就越重要），其实不然，单纯使用 $TF$ 来评估关键词的重要性忽略了常用词的干扰。常用词就是指那些文章中大量用到的，但是不能反映文章性质的那种词，比如：因为、所以、因此等等的连词，在英文文章里就体现为`and`、`the`、`of`等等的词。这些词往往拥有较高的 $TF$ ，所以仅仅使用 $TF$来考察一个词的关键性，是不够的。这里我们要引出 $IDF$ ，来帮助我们解决这个问题。

## IDF
$IDF$，英文全称：`Inverse Document Frequency`，即“反文档频率”。先看什么是文档频率，文档频率 $DF$ 就是一个词在整个文库词典中出现的频率，就拿上一个例子来讲：一个文件集中有 100 篇文章，共有10篇文章包含`“机器学习”`这个词，那么它的文档频率就是$\frac{10}{100}=0.1$，反文档频率$IDF$就是这个值的倒数，即10。因此得出它的计算公式:  

$$IDF_i=log\frac{|D|}{|j:t_i \in d_j|+1}$$  

其中：
>$IDF_i$: 词语 $i$ 的反文档频率  
> $|D|$:   语料库中的文件总数  
> $|j:t_i \in d_j|$: 出现词语 $i$ 的文档总数.  
> $+1$: 为了防止分母为0  

所以结合$TF$和$IDF$就能评估一个词对于文章的重要程度。具体即是使用 $TF\times IDF$ 来量化词语重要程度，越大代表相关度越高。  
还是用上面的例子，我们来直观地看看 $IDF$ 是如何消去常用词干扰的。  

假设100篇文档有10000个词，研究某篇500词文章，`“机器学习”`出现了20次，`“而且”`出现了20次，那么他们的TF都是 $\frac{20}{500}=0.4$。再来看 $IDF$，对于语料库的100篇文章，每篇都出现了`“而且”`，因此它的 $IDF$就是 $log1=0$,他的 $TF\times IDF$=0。而`“机器学习”`出现了10篇，那么它的IDF就是$log10=1$,他的$TF\times IDF=0.04>0$，显然`“机器学习”`比`“而且”`更加重要(对这篇文章的主题相关性来说)。  

## 总结
这算法看似简单，实际上在SEO搜索引擎优化啊，文本分类方面用的挺多的，面试时也常常作为信息论知识储备来出题。
